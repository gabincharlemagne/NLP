{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUYjLYt9IQom"
      },
      "source": [
        "# Word Embeddings Première étape : la data préparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW6C2dPQJmIn",
        "outputId": "5365786d-cf90-469e-ed53-6c559b679f43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/586.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/586.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTO8SRxHITN8",
        "outputId": "e9e7e745-ad7b-4e35-97a3-b88f7dfdafcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import emoji\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from scipy import linalg\n",
        "from collections import defaultdict"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tuvG4mUJ5hv",
        "outputId": "d7b8231a-7818-4573-bb9d-60abcb5e9888",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COSjtwLlIdBZ"
      },
      "source": [
        "# Data préparation\n",
        "\n",
        "Dans la phase de préparation des données, en commençant par un corpus de texte, vous:\n",
        "\n",
        "- Nettoyer et marquer le corpus.\n",
        "\n",
        "- Extraire les paires de mots de contexte et de mot central qui constitueront l'ensemble des données d'entraînement pour le modèle CBOW. Les mots contextuels sont les caractéristiques qui seront introduites dans le modèle, et les mots centraux sont les valeurs cibles que le modèle apprendra à prédire.\n",
        "\n",
        "- Créez des représentations vectorielles simples des mots de contexte (caractéristiques) et des mots centraux (cibles) qui peuvent être utilisés par le réseau neuronal du modèle CBOW."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZAMlED9IgbR"
      },
      "source": [
        "## Cleaning et tokenization\n",
        "\n",
        "Pour démontrer le processus de nettoyage et de tokenisation, considérons un corpus qui contient des emojis et divers signes de ponctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FYNSCL8I_Hu"
      },
      "source": [
        "# Define a corpus\n",
        "corpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSQ14viHJDML"
      },
      "source": [
        "Premièrement, remplacez tous les signes de ponctuation interruptifs - tels que les virgules et les points d'exclamation - par des points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lzLIPQRJLa0",
        "outputId": "3899da46-1827-44fe-bb63-6a592b7a49f3"
      },
      "source": [
        "# Print original corpus\n",
        "print(f'Corpus:  {corpus}')\n",
        "\n",
        "# Do the substitution\n",
        "### A modifier - debut\n",
        "data = re.sub(r'[!?]+', '.', corpus)\n",
        "### A modifier - fin\n",
        "\n",
        "# Print cleaned corpus\n",
        "print(f'After cleaning punctuation:  {data}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\n",
            "After cleaning punctuation:  Who ❤️ \"word embeddings\" in 2020. I do.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-noUOlB1Ja3a"
      },
      "source": [
        "Ensuite, utilisez le moteur de tokenisation de NLTK pour diviser le corpus en tokens individuels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dR32WmfJP4q",
        "outputId": "175702d6-76ca-422f-fa07-b1b2b6accb65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print cleaned corpus\n",
        "print(f'Initial string:  {data}')\n",
        "\n",
        "# Tokenize the cleaned corpus\n",
        "### A modifier - debut\n",
        "data = word_tokenize(data)\n",
        "### A modifier - fin\n",
        "\n",
        "# Print the tokenized version of the corpus\n",
        "print(f'After tokenization:  {data}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial string:  Who ❤️ \"word embeddings\" in 2020. I do.\n",
            "After tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z644AnwYJ8Zx"
      },
      "source": [
        "Enfin, comme vous l'avez vu dans le cours, débarrassez-vous des chiffres et de la ponctuation autres que les points, et convertissez tous les tokens restants en minuscules.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTn9aw9mKLo0",
        "outputId": "10d778c1-0c07-458d-b6a1-2c6976302bf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print the tokenized version of the corpus\n",
        "print(f'Initial list of tokens:  {data}')\n",
        "\n",
        "# Filter tokenized corpus using list comprehension\n",
        "### A modifier - debut\n",
        "# Éliminer les chiffres et la ponctuation autre que les points, et convertir en minuscules\n",
        "data = [re.sub(r'(?<!\\S)[^a-z\\.\\s❤️]+(?!\\S)', '', token.lower()) for token in data]\n",
        "\n",
        "# Filtrer les tokens vides résultant du nettoyage\n",
        "data = [token for token in data if token]\n",
        "### A modifier - fin\n",
        "\n",
        "# Print the tokenized and filtered version of the corpus\n",
        "print(f'After cleaning:  {data}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n",
            "After cleaning:  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjxModWSKNe1"
      },
      "source": [
        "Notez que l'émoji du coeur est considéré comme un token comme n'importe quel mot normal.\n",
        "\n",
        "Maintenant, rationalisons le processus de nettoyage et de tokenisation en intégrant les étapes précédentes dans une fonction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRusZszFKUrS"
      },
      "source": [
        "# Define the 'tokenize' function that will include the steps previously seen\n",
        "def tokenize(corpus):\n",
        "\n",
        "    # Tokeniser le corpus en utilisant NLTK\n",
        "    tokens = word_tokenize(corpus)\n",
        "\n",
        "    # Nettoyer les tokens en éliminant les chiffres et la ponctuation autre que les points et les emojis, et convertir en minuscules\n",
        "    data = [\n",
        "        re.sub(r'(?<!\\S)[^a-z\\.\\s❤️]+(?!\\S)', '', token.lower())\n",
        "        for token in tokens\n",
        "        if not re.match(r'^\\d+$', token)\n",
        "    ]\n",
        "\n",
        "    # Filtrer les tokens vides résultant du nettoyage\n",
        "    data = [token for token in data if token]\n",
        "    return data"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bplJ85lRKXFv"
      },
      "source": [
        "Appliquez cette fonction au corpus sur lequel vous allez travailler dans le reste de ce carnet : \"I am happy because I am learning\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzwzPIRvKgnY",
        "outputId": "dda84c50-e55e-4a00-9a55-0685cb7f93e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define new corpus\n",
        "corpus = 'I am happy because I am learning'\n",
        "\n",
        "# Print new corpus\n",
        "print(f'Corpus:  {corpus}')\n",
        "\n",
        "# Save tokenized version of corpus into 'words' variable\n",
        "words = tokenize(corpus)\n",
        "\n",
        "# Print the tokenized version of the corpus\n",
        "print(f'Words (tokens):  {words}')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus:  I am happy because I am learning\n",
            "Words (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV0iN9f-Kkca"
      },
      "source": [
        "**Maintenant, essayez-le vous-même avec votre propre phrase.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1fhr6u9KqS2",
        "outputId": "22ca7783-1ced-4734-b41c-3e5c535f0d13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Run this with any sentence\n",
        "tokenize(\"Now it's your turn: try with your own sentence!\")\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['now', 'it', \"'s\", 'your', 'turn', 'try', 'with', 'your', 'own', 'sentence']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdG8ZSitKvoq"
      },
      "source": [
        "## Sliding window of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGG8MCBUKzw8"
      },
      "source": [
        "Maintenant que vous avez transformé le corpus en une liste de token propres, vous pouvez faire glisser une fenêtre de mots sur cette liste. Pour chaque fenêtre, vous pouvez extraire un mot central et les mots de contexte.\n",
        "\n",
        "La fonction `get_windows` dans la cellule suivante a été introduite dans le cours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGqBCwEsK-bA"
      },
      "source": [
        "Le premier argument de cette fonction est une liste de mots (ou de tokens). Le deuxième argument, \"C\", est le contexte demi-taille. Rappelons que pour un mot central donné, les mots de contexte sont constitués de mots \"C\" à gauche et de mots \"C\" à droite du mot central.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8izH0tVbK8Xm"
      },
      "source": [
        "# Define the 'get_windows' function\n",
        "def get_windows(words, C):\n",
        "    i = C\n",
        "    while i < len(words) - C:\n",
        "\n",
        "        ### A modifier - debut\n",
        "        center_word = words[i]\n",
        "        context_words = words[i - C:i] + words[i + 1:i + C + 1]\n",
        "        ### A modifier - fin\n",
        "        yield context_words, center_word\n",
        "        i += 1"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8RTPyXQLYaA"
      },
      "source": [
        "Voici comment vous pouvez utiliser cette fonction pour extraire les mots de contexte et les mots centraux d'une liste de tokens. Ces mots de contexte et mots centraux constitueront le jeu de formation que vous utiliserez pour former le modèle CBOW."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLOalYubLJ2X",
        "outputId": "d7294ce4-0bad-49b8-d049-aa63b9504d6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print 'context_words' and 'center_word' for the new corpus with a 'context half-size' of 2\n",
        "for x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n",
        "    print(f'{x}\\t{y}')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'because', 'i']\thappy\n",
            "['am', 'happy', 'i', 'am']\tbecause\n",
            "['happy', 'because', 'am', 'learning']\ti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkbagsC1LZwd"
      },
      "source": [
        "Le premier exemple de l'ensemble de formation est constitué de\n",
        "\n",
        "- les mots de contexte \"I\", \"am\", \"because\", \"I\",\n",
        "\n",
        "- et le mot central à prédire : \"happy\".\n",
        "\n",
        "**Maintenant, essayez-le vous-même. Dans la cellule suivante, vous pouvez changer la phrase et l'hyperparamètre C."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8Dc6AwqLqZH",
        "outputId": "fcc6a748-ba4b-4524-80bf-c8b9fa9ab637",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print 'context_words' and 'center_word' for any sentence with a 'context half-size' of 1\n",
        "for x, y in get_windows(tokenize(\"This year at Christmas i'd got : a car, and some money!\"), 2):\n",
        "    print(f'{x}\\t{y}')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'year', 'christmas', 'i']\tat\n",
            "['year', 'at', 'i', \"'d\"]\tchristmas\n",
            "['at', 'christmas', \"'d\", 'got']\ti\n",
            "['christmas', 'i', 'got', 'a']\t'd\n",
            "['i', \"'d\", 'a', 'car']\tgot\n",
            "[\"'d\", 'got', 'car', 'and']\ta\n",
            "['got', 'a', 'and', 'some']\tcar\n",
            "['a', 'car', 'some', 'money']\tand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9hpCNfCLsaH"
      },
      "source": [
        "## Transformer les mots en vecteurs pour l'entraînement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhlL5fI_MzvZ"
      },
      "source": [
        "Pour terminer la préparation de l'ensemble du jeu d'entraînement, il faut transformer les mots de contexte et les mots centraux en vecteurs.\n",
        "\n",
        "### Mise en correspondance des mots avec les indices et des indices avec les mots\n",
        "\n",
        "Les mots centraux seront représentés sous forme de vecteurs one-hot, et les vecteurs qui représentent les mots de contexte sont également basés sur des vecteurs one-hot.\n",
        "\n",
        "Pour créer des vecteurs de mots uniques, vous pouvez commencer par faire correspondre chaque mot unique à un entier (ou index) unique. Je vous ai fourni une fonction d'aide, `get_dict`, qui crée un dictionnaire Python qui fait correspondre les mots à des entiers et vice-versa.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0LeR3yvN6Rt"
      },
      "source": [
        "def get_dict(data):\n",
        "   \"\"\"    Input:\n",
        "           K: the number of negative samples\n",
        "           data: the data you want to pull from\n",
        "           indices: a list of word indices\n",
        "         Output:\n",
        "           word_dict: a dictionary with the weighted probabilities of each word\n",
        "           word2Ind: returns dictionary mapping the word to its index\n",
        "           Ind2Word: returns dictionary mapping the index to its word\n",
        "   \"\"\"\n",
        "\n",
        "   words = sorted(list(set(data)))\n",
        "   n = len(words)\n",
        "   idx = 0\n",
        "\n",
        "   # return these correctly\n",
        "   word2Ind = {}\n",
        "   Ind2word = {}\n",
        "   for k in words:\n",
        "     word2Ind[k] = idx\n",
        "     Ind2word[idx] = k\n",
        "     idx += 1\n",
        "   return word2Ind, Ind2word"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA-DVPC7Plj3"
      },
      "source": [
        "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
        "word2Ind, Ind2word = get_dict(words)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D61flJ_5PnpV"
      },
      "source": [
        "Voici le dictionnaire qui fait correspondre les mots à des indices numériques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZJhXw9NPq1R",
        "outputId": "28a5968e-3469-448b-8510-dc28c34e3d90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print 'word2Ind' dictionary\n",
        "word2Ind"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcsFqkpCR5yZ"
      },
      "source": [
        "Vous pouvez utiliser ce dictionnaire pour obtenir l'index d'un mot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgB82tBfR75O",
        "outputId": "21dd9505-0d3a-4fdf-96be-b103c387b6dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print value for the key 'i' within word2Ind dictionary\n",
        "print(\"Index of the word 'i':  \",word2Ind['i'])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index of the word 'i':   3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBrFiiS6SdK7"
      },
      "source": [
        "Et inversement, voici le dictionnaire qui fait correspondre les index aux mots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L11RSBZ2Sfsc",
        "outputId": "529503ba-b3e7-42c1-eb46-014c372874cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print 'Ind2word' dictionary\n",
        "Ind2word"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UTlx2tSSipB",
        "outputId": "3b666b38-68b4-48d9-f2ac-d8c1f4f31fa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print value for the key '2' within Ind2word dictionary\n",
        "print(\"Word which has index 2:  \",Ind2word[2] )"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word which has index 2:   happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4ZR6TN4SpNP"
      },
      "source": [
        "Enfin, obtenez la longueur de l'un ou l'autre de ces dictionnaires pour connaître la taille du vocabulaire de votre corpus, c'est-à-dire le nombre de mots différents composant le corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wTLCgCdSqxJ",
        "outputId": "9739b4c1-5298-4d8f-db04-a0d298e6eb44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Save length of word2Ind dictionary into the 'V' variable\n",
        "V = len(word2Ind)\n",
        "\n",
        "# Print length of word2Ind dictionary\n",
        "print(\"Size of vocabulary: \", V)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary:  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zKcIIqUSvET"
      },
      "source": [
        "### Obtenir des one-hot vecteurs\n",
        "\n",
        "Rappelez-vous que vous pouvez facilement convertir un entier, $n$, en un vecteur d'un seul coup.\n",
        "\n",
        "Considérez le mot \"happy\". Tout d'abord, récupérez son index numérique."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n1KkYDeS_JX",
        "outputId": "60b82b5c-af04-4a2a-d371-8cbdce3eff7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Save index of word 'happy' into the 'n' variable\n",
        "n = word2Ind['happy']\n",
        "\n",
        "# Print index of word 'happy'\n",
        "n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzp_pgW9TFFc"
      },
      "source": [
        "Créez maintenant un vecteur de la taille du vocabulaire, et remplissez-le avec des zéros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isnZHG5ATKXl",
        "outputId": "c1953508-ceb6-44a0-fb2c-5ad3f1562a2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create vector with the same length as the vocabulary, filled with zeros\n",
        "### A modifier - debut\n",
        "center_word_vector = np.zeros(V)\n",
        "### A modifier - fin\n",
        "\n",
        "# Print vector\n",
        "center_word_vector"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z4UWky8TMl4"
      },
      "source": [
        "Vous pouvez confirmer que le vecteur a la bonne taille."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10grSJfMTQkL",
        "outputId": "0910e531-1582-40ae-e10c-4f88595e3a8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Assert that the length of the vector is the same as the size of the vocabulary\n",
        "len(center_word_vector) == V"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_8btBlLTRta"
      },
      "source": [
        "Ensuite, remplacez le 0 de l'élément $n$-th par un 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP_ZDt-6Tbgc"
      },
      "source": [
        "# Replace element number 'n' with a 1\n",
        "center_word_vector[n] = 1"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k02E5K_0TeEI"
      },
      "source": [
        "Et vous avez votre one-hot vecteur pour votre mot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7WoEkuNTl3X",
        "outputId": "9e92c00e-e535-4e34-c567-62f229053f6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print vector\n",
        "center_word_vector"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DjXNo12TpJ6"
      },
      "source": [
        "**Vous pouvez maintenant regrouper toutes ces étapes dans une fonction pratique, qui prend comme paramètres : un mot à encoder, un dictionnaire qui associe les mots à des index, et la taille du vocabulaire.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OCHyb1vTvXM"
      },
      "source": [
        "# Define the 'word_to_one_hot_vector' function that will include the steps previously seen\n",
        "def word_to_one_hot_vector(word, word2Ind, V):\n",
        "\n",
        "    ### A modifier - debut\n",
        "    one_hot_vector = np.zeros(V)\n",
        "    one_hot_vector[word2Ind[word]] = 1\n",
        "    ### A modifier - fin\n",
        "    return one_hot_vector"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1VSdiRKTxXc"
      },
      "source": [
        "Vérifiez qu'il fonctionne comme prévu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlteDjq0T1du",
        "outputId": "787c7cf3-2505-47c1-e23f-ef8219da8564",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print output of 'word_to_one_hot_vector' function for word 'happy'\n",
        "word_to_one_hot_vector('happy', word2Ind, V)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGHMFiZmT6R-"
      },
      "source": [
        "**Quel est le mot vecteur pour \"learning\"?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7qT7f1sT9ks",
        "outputId": "a191036d-c7af-4e90-975b-91ac7d41af7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print output of 'word_to_one_hot_vector' function for word 'learning'\n",
        "word_to_one_hot_vector('learning', word2Ind, V)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XbNd7WbUBr3"
      },
      "source": [
        "Réponse attendue :\n",
        "\n",
        "    array([0., 0., 0., 0., 1.])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFyPRmqiUIJO"
      },
      "source": [
        "### Obtenir des vecteurs de mots contextuels\n",
        "\n",
        "Pour créer les vecteurs qui représentent les mots de contexte, vous calculerez la moyenne des one-hot vecteurs représentant les mots individuels.\n",
        "\n",
        "Commençons par une liste de mots contextuels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fadSaWV6UH3s"
      },
      "source": [
        "# Define list containing context words\n",
        "context_words = ['i', 'am', 'because', 'i']"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJNExv6_UdR2"
      },
      "source": [
        "En utilisant la fonction `word_to_one_hot_vector` que vous avez créée dans la section précédente, vous pouvez créer une liste de vecteurs one-hot représentant chacun des mots du contexte."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZPlZWI1UpRl",
        "outputId": "e795cbfc-8f23-4dba-b281-fc0154600932",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create one-hot vectors for each context word using list comprehension\n",
        "\n",
        "### A modifier - debut\n",
        "context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
        "### A modifier - fin\n",
        "\n",
        "# Print one-hot vectors for each context word\n",
        "context_words_vectors"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0., 0., 0., 1., 0.]),\n",
              " array([1., 0., 0., 0., 0.]),\n",
              " array([0., 1., 0., 0., 0.]),\n",
              " array([0., 0., 0., 1., 0.])]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5TlknIiUv_E"
      },
      "source": [
        "Et vous pouvez maintenant simplement obtenir la moyenne de ces vecteurs en utilisant la fonction `mean` de numpy, pour obtenir la représentation vectorielle des mots de contexte."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeF2X3kNUxhL",
        "outputId": "18486b2a-49c8-4a5d-981f-0f1f7aa02e58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Compute mean of the vectors using numpy\n",
        "np.mean(context_words_vectors, axis=0)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me5H1I3OU5th"
      },
      "source": [
        "Notez le paramètre `axis=0` de la fonction `mean` est utilisé pour calculer la moyenne des lignes (si vous aviez voulu la moyenne des colonnes, vous auriez utilisé `axis=1`).\n",
        "\n",
        "**Créez maintenant la fonction `context_words_to_vector` qui prend en compte une liste de mots de contexte, un dictionnaire de mots à indexer, et une taille de vocabulaire, et produit la représentation vectorielle des mots de contexte.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qiJE7ZyUxeG"
      },
      "source": [
        "# Define the 'context_words_to_vector' function that will include the steps previously seen\n",
        "def context_words_to_vector(context_words, word2Ind, V):\n",
        "\n",
        "    ### A modifier - debut\n",
        "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
        "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
        "    ### A modifier - fin\n",
        "    return context_words_vectors"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2ZfSB69VHDh"
      },
      "source": [
        "Et vérifiez que vous obtenez le même résultat que l'approche manuelle ci-dessus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4atGVPHVLeQ",
        "outputId": "50286f97-44a7-471d-fb7e-a5090eea8bd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print output of 'context_words_to_vector' function for context words: 'i', 'am', 'because', 'i'\n",
        "context_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIxFJnDaVNUa"
      },
      "source": [
        "**Quelle est la représentation vectorielle des mots du contexte \"am happy i am\" ?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1isiPPeEVZDc",
        "outputId": "146012a7-1cac-45db-c003-94908656a14e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print output of 'context_words_to_vector' function for context words: 'am', 'happy', 'i', 'am'\n",
        "context_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.5 , 0.  , 0.25, 0.25, 0.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBsta4DoVZ-D"
      },
      "source": [
        "Réponse attendue:\n",
        "\n",
        "    array([0.5 , 0.  , 0.25, 0.25, 0.  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNyO1D85VisQ"
      },
      "source": [
        "## Construire l'ensemble du training set\n",
        "\n",
        "Vous pouvez maintenant combiner les fonctions que vous avez créées dans les sections précédentes, afin de construire le jeu d'entraînement pour le modèle CBOW, à partir du corpus suivant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSz0IQk6Vta3",
        "outputId": "a5aacdb3-3a30-4659-f553-2b1064558c2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print corpus\n",
        "words"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'happy', 'because', 'i', 'am', 'learning']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7ID3-DhV1Gr"
      },
      "source": [
        "Pour ce faire, vous devez utiliser la fonction de fenêtre coulissante (`get_windows`) pour extraire les mots de contexte et les mots de centre, et vous devez ensuite convertir ces ensembles de mots en une représentation vectorielle de base en utilisant `word_to_one_hot_vector` et `context_words_to_vector`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQe1OIeMWDQl",
        "outputId": "d6c604ea-f697-4bb2-a7fb-3a6006702cdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print vectors associated to center and context words for corpus\n",
        "for context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n",
        "    print(f'Context words:  {context_words} -> {context_words_to_vector(context_words, word2Ind, V)}')\n",
        "    print(f'Center word:  {center_word} -> {word_to_one_hot_vector(center_word, word2Ind, V)}')\n",
        "    print()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context words:  ['i', 'am', 'because', 'i'] -> [0.25 0.25 0.   0.5  0.  ]\n",
            "Center word:  happy -> [0. 0. 1. 0. 0.]\n",
            "\n",
            "Context words:  ['am', 'happy', 'i', 'am'] -> [0.5  0.   0.25 0.25 0.  ]\n",
            "Center word:  because -> [0. 1. 0. 0. 0.]\n",
            "\n",
            "Context words:  ['happy', 'because', 'am', 'learning'] -> [0.25 0.25 0.25 0.   0.25]\n",
            "Center word:  i -> [0. 0. 0. 1. 0.]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--8izoypWE83"
      },
      "source": [
        "Dans ce notebook, vous effectuerez une seule itération du jeu d'entraînement en utilisant un seul exemple, mais dans le cadre d'un projet, vous formerez le modèle CBOW en utilisant plusieurs itérations et différent batchs d'exemples.\n",
        "\n",
        "Voici comment vous utiliserez une fonction de générateur Python (rappelez-vous le mot-clé \"[yield](https://fr.wikipedia.org/wiki/Yield_(instruction))\") pour faciliter l'itération sur un ensemble d'exemples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVwacU9pWc2v"
      },
      "source": [
        "# Define the generator function 'get_training_example'\n",
        "def get_training_example(words, C, word2Ind, V):\n",
        "    ### A modifier - debut\n",
        "    for context_words, center_word in get_windows(words, C):\n",
        "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n",
        "    ### A modifier - fin"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30yQDjsWWpRc"
      },
      "source": [
        "La sortie de cette fonction peut être répétée pour obtenir des vecteurs de mots de contexte et des vecteurs de mots centraux successifs, comme démontré dans la cellule suivante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ah76UqWuz8",
        "outputId": "70006424-a9e3-4ed2-b9a4-eccc535b7287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print vectors associated to center and context words for corpus using the generator function\n",
        "for context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n",
        "    print(f'Context words vector:  {context_words_vector}')\n",
        "    print(f'Center word vector:  {center_word_vector}')\n",
        "    print()\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context words vector:  [0.25 0.25 0.   0.5  0.  ]\n",
            "Center word vector:  [0. 0. 1. 0. 0.]\n",
            "\n",
            "Context words vector:  [0.5  0.   0.25 0.25 0.  ]\n",
            "Center word vector:  [0. 1. 0. 0. 0.]\n",
            "\n",
            "Context words vector:  [0.25 0.25 0.25 0.   0.25]\n",
            "Center word vector:  [0. 0. 0. 1. 0.]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bkw_snnWyWW"
      },
      "source": [
        "Votre jeu d'entraînement est prêt, vous pouvez maintenant passer au modèle CBOW.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1Ibqk53W8mW"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}